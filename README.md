# üé¨ Marapthon Season 2: Content Insights & Viewer Trends

This project analyzes viewer engagement and content performance during **Marapthon Season 2** on YouTube, focusing on identifying trends over time and content strategies that drive higher views.

---

## üöÄ Project Overview

Using the YouTube Data API, this pipeline collects video metadata and performance statistics, stores raw and transformed data in **Google Cloud Storage** and **BigQuery**, and visualizes insights in **Looker Studio**. The pipeline is fully orchestrated with **Apache Airflow**, and infrastructure is provisioned using **Terraform**.

---

## üß© Key Features

- üì• **Batch Data Ingestion**: Automatically fetches daily YouTube data using the YouTube Data API.
- ‚òÅÔ∏è **Cloud-native**: Runs on **Google Cloud Platform** with infrastructure as code (IaC) using Terraform.
- üèóÔ∏è **Workflow Orchestration**: Full ETL pipeline built with Apache Airflow (Dockerized).
- üßÆ **Data Warehouse**: Optimized partitioned tables stored in **BigQuery**.
- üîÑ **Transformations**: Performed using **dbt** for clean, modular SQL models.
- üìä **Visualization**: Looker Studio dashboard showing:
  - Day-of-week viewership trends
  - Top performing videos
  - Audience behavior during the season

---

## üìÅ Project Structure

```
.
‚îú‚îÄ‚îÄ dags/                     # Airflow DAGs
‚îú‚îÄ‚îÄ dbt/                      # dbt models and configs
‚îú‚îÄ‚îÄ terraform/                # Terraform infrastructure config
‚îú‚îÄ‚îÄ creds.json                # GCP credentials (not tracked in Git)
‚îú‚îÄ‚îÄ docker-compose.yml        # Docker services for Airflow & Terraform
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îî‚îÄ‚îÄ README.md
```

---

## üíª How to Use

### 1. Clone the repository

```bash
git clone https://github.com/yourusername/youtube-data-analysis.git
cd youtube-data-analysis
```

### 2. Add your Google Cloud credentials

Place your service account key as `creds.json` in the project root (same level as `docker-compose.yml`).

> ‚ö†Ô∏è Be sure to **add `creds.json` to `.gitignore`** and never commit it.

### 3. Provision infrastructure with Terraform

```bash
docker-compose run terraform
```

This will create:
- GCS bucket
- BigQuery dataset
- Service setup for the pipeline

### 4. Build and run Airflow

```bash
docker-compose up --build
```

Airflow web UI will be available at [http://localhost:8080](http://localhost:8080)  
Default login:  
- **Username**: `admin`  
- **Password**: `admin`

### 5. Trigger the DAG

Go to the Airflow UI, enable and trigger the DAG manually or wait for its schedule to kick in.

### 6. View the dashboard

Once your data is in BigQuery and transformed via dbt, view the insights in Looker Studio using your connected project.

---

## üõ†Ô∏è Tech Stack

- **GCP**: Google Cloud Storage, BigQuery
- **Apache Airflow** (with Docker)
- **Terraform**
- **dbt (Data Build Tool)**
- **Looker Studio**
- **Python**: `requests`, `pandas`, `google-api-python-client`, `google-cloud-storage`, etc.

---

## üìä Dashboard

You can explore the interactive data report here:  
üëâ [Marapthon Season 2 Content Insights & Viewer Trends](https://lookerstudio.google.com/reporting/5b7aa03c-05a0-4880-8946-20ca911f9f0b)

This dashboard visualizes:

- Top-performing live based on view trends
- Viewer behavior across days
- Shifts in content engagement over time
- Daily view patterns

---
